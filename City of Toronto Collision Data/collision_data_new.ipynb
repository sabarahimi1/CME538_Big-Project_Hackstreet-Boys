{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d397b5ec",
   "metadata": {},
   "source": [
    "## City of Toronto Collisions Data\n",
    "\n",
    "\n",
    "The Total Collisions Dataset is a CSV file containing detailed records of motor vehicle collisions within the City of Toronto. The dataset uses the WGS84 Coordinate Reference System, ensuring consistent geographic representation of collision locations. Key attributes include the geographic location of each collision, whether it resulted in a fatality or injury, and the timestamp of the event. For our analysis, we will focus on data from 2021 to 2024 to align with recent census data, providing insights into contemporary trends and patterns in collisions. Additionally, the dataset may include supplementary fields such as road conditions, weather visibility, and types of vehicles involved, offering a comprehensive view of the contributing factors to these incidents. By analyzing this dataset, we aim to identify high-risk areas and underlying causes of collisions to inform preventative strategies and improve road safety."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fed1ab22",
   "metadata": {},
   "source": [
    "## Setup Notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4413bbb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import 3rd party libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pylab as plt\n",
    "import geopandas as gpd\n",
    "import matplotlib.pyplot as plt\n",
    "import folium\n",
    "from IPython.display import display\n",
    "import geopandas as gpd\n",
    "from shapely.geometry import Point\n",
    "\n",
    "# Configure Notebook\n",
    "%matplotlib inline\n",
    "plt.style.use('fivethirtyeight')\n",
    "sns.set_context(\"notebook\")\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94bbfd2a",
   "metadata": {},
   "source": [
    "## Import GeoJson Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a1ee6f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a base map\n",
    "map_2 = folium.Map(location=[43.6426, -79.3871], \n",
    "                   tiles='OpenStreetMap', \n",
    "                   zoom_start=10)\n",
    "\n",
    "# Correct the GeoJSON file path\n",
    "geojson_file_path = \"/Users/aaliyashaikh/Desktop/Traffic_Collisions_Open_Data_-6024052409346627848.geojson\"\n",
    "\n",
    "# Ensure the file exists at the specified path\n",
    "try:\n",
    "    folium.GeoJson(geojson_file_path, name=\"Collision Data\").add_to(map_2)\n",
    "    # Display the map\n",
    "    map_2\n",
    "except FileNotFoundError:\n",
    "    print(f\"Error: The file '{geojson_file_path}' was not found. Please verify the path.\")\n",
    "\n",
    "# Display the map directly in the notebook\n",
    "display(map_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "825a90b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import dataset as a DataFrane\n",
    "collision_data = gpd.read_file('FATALS_KSI_4359710384762535516.geojson')\n",
    "\n",
    "# View DataFrame\n",
    "collision_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "179fde40",
   "metadata": {},
   "source": [
    "## Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c03c295a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the number of columns and rows\n",
    "collision_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7bfb0a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the columns in DataFrame\n",
    "collision_data.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d061c546",
   "metadata": {},
   "source": [
    "Based on the Toronto Police Service's Traffic Collisions Open Data (ASR-T-TBL-001), here is a description of each column in the dataset:\n",
    "\n",
    "- OBJECTID: A unique identifier for each collision record.\n",
    "- EVENT_UNIQUE_ID: A unique code assigned to each specific event.\n",
    "- OCC_DATE: The date and time of the collision occurrence.\n",
    "- OCC_MONTH: The month in which the collision occurred.\n",
    "- OCC_DOW: The day of the week on which the collision happened.\n",
    "- OCC_YEAR: The year in which the collision occurred.\n",
    "- OCC_HOUR: The hour of the day (24-hour format) when the collision took place.\n",
    "- DIVISION: The division code where the incident was recorded.\n",
    "- FATALITIES: The number of fatalities resulting from the collision.\n",
    "- INJURY_COLLISIONS: Indicates whether the collision involved injuries (e.g., \"YES\" or \"NO\").\n",
    "- FTR_COLLISIONS: Indicates whether the collision was a \"Fail to Remain\" incident.\n",
    "- PD_COLLISIONS: Indicates whether the collision involved property damage.\n",
    "- HOOD_158: Numeric code representing the neighborhood where the collision occurred.\n",
    "- NEIGHBOURHOOD_158: The name of the neighborhood and its associated number.\n",
    "- LONG_WGS84: Longitude of the collision location (in WGS84 coordinate system).\n",
    "- LAT_WGS84: Latitude of the collision location (in WGS84 coordinate system).\n",
    "- AUTOMOBILE: Indicates if an automobile was involved in the collision (e.g., \"YES\" or \"NO\").\n",
    "- MOTORCYCLE: Indicates if a motorcycle was involved in the collision.\n",
    "- PASSENGER: Indicates if a passenger vehicle was involved in the collision.\n",
    "- BICYCLE: Indicates if a bicycle was involved in the collision.\n",
    "- PEDESTRIAN: Indicates if a pedestrian was involved in the collision.\n",
    "- x: A spatial coordinate (likely projected coordinate system) for mapping the location.\n",
    "- y: Another spatial coordinate for mapping the location."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d96cff4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check data types per column\n",
    "print(collision_data.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a46e8652",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check numerical statistics for each column\n",
    "collision_data.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d0411c5",
   "metadata": {},
   "source": [
    "When analyzing collision data, the columns to drop depend on your analysis goals. However, some columns may be less relevant or redundant for most collision-related analyses. Here’s a suggestion of columns you can drop and why:\n",
    "\n",
    "Columns that can be dropped:\n",
    "- OBJECTID: A unique identifier that is not informative for analysis.\n",
    "- EVENT_UNIQUE_ID: Similar to OBJECTID, this is unique to each record and not relevant for statistical or pattern analysis.\n",
    "- HOOD_158: Numeric code for neighborhoods is redundant when the neighborhood name (NEIGHBOURHOOD_158) is available.\n",
    "- x and y: Projected spatial coordinates may be redundant if LONG_WGS84 and LAT_WGS84 are present and sufficient for geospatial analysis.\n",
    "- OCC_MONTH: The month can be derived from the date in OCC_DATE if needed.\n",
    "- OCC_YEAR: Similarly, the year can also be derived from OCC_DATE.\n",
    "- DIVISION: This may be redundant if neighborhood-level analysis is sufficient or if it doesn’t contribute to your objectives.\n",
    "- PASSENGER: This column may overlap with AUTOMOBILE since most passenger vehicles are classified as automobiles. You can keep one depending on your needs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42dbf187",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop columns \n",
    "collision_data = collision_data.drop(columns = ['OBJECTID', 'EVENT_UNIQUE_ID', 'HOOD_158', 'x', 'y',\n",
    "                                      'OCC_MONTH', 'OCC_YEAR', 'DIVISION', 'PASSENGER', 'AUTOMOBILE',\n",
    "                                      'FTR_COLLISIONS', 'PD_COLLISIONS', 'MOTORCYCLE'], errors='ignore')\n",
    "\n",
    "# Check if columns are removed\n",
    "collision_data.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c7d9b2e",
   "metadata": {},
   "source": [
    "## Data Cleaning\n",
    "\n",
    "Data cleaning for the collision dataset ensures:\n",
    "\n",
    "- Accuracy: Removes errors and inconsistencies.\n",
    "- Efficiency: Streamlines the dataset for quicker and easier analysis.\n",
    "- Reliability: Produces trustworthy insights and recommendations.\n",
    "- Focus: Tailors the data for the specific analysis of collision patterns and trends."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8b752d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for missing values\n",
    "print(collision_data.isnull().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ac0e191",
   "metadata": {},
   "source": [
    "Handling missing values should be approached differently for numerical and categorical data to ensure the dataset's integrity and reliability. For numerical columns, such as LATITUDE, LONGITUDE, missing values can be imputed using statistically meaningful methods like the mean or median, depending on the distribution. This ensures that numerical characteristics are preserved without introducing bias, allowing smooth analysis and modeling. However, for categorical columns like AUTOMOBILE, OCC_DATE, or MOTORCYCLE, missing values often carry contextual meaning, such as unrecorded or unknown data. Imputing these values with the most frequent category (mode) could distort the actual patterns and introduce bias. Instead, categorical columns are better handled by retaining NaN values or replacing them with a placeholder like \"Unknown\" or \"Not Recorded\", preserving their contextual integrity. By treating these data types differently, we ensure accurate and meaningful analysis without compromising the dataset's validity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57edfc13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to handle missing data for both numerical and categorical columns\n",
    "def handle_missing_data(collision_data):\n",
    "    for column in collision_data.columns:\n",
    "        if collision_data[column].isnull().sum() > 0:  # Check for missing values\n",
    "            if pd.api.types.is_numeric_dtype(collision_data[column]):\n",
    "                # Numerical data: Use mean or median based on skewness\n",
    "                skewness = collision_data[column].skew()\n",
    "                if abs(skewness) < 0.5:  # Normally distributed\n",
    "                    impute_value = collision_data[column].mean()\n",
    "                    print(f\"Imputing missing values in numerical column '{column}' with mean: {impute_value:.2f}\")\n",
    "                else:  # Skewed distribution\n",
    "                    impute_value = collision[column].median()\n",
    "                    print(f\"Imputing missing values in numerical column '{column}' with median: {impute_value:.2f}\")\n",
    "                collision_data[column].fillna(impute_value, inplace=True)\n",
    "            \n",
    "    return collision_data\n",
    "\n",
    "# Handle missing data\n",
    "collision_handled = handle_missing_data(collision_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cdb7685",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify that missing values have been handled\n",
    "print(collision_handled.isnull().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "706d8b6f",
   "metadata": {},
   "source": [
    "All numerical missing (null) values have been dealt with (removed or imputed). Let's also remove all duplicates. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45f15d28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove Duplicates\n",
    "collision_data.drop_duplicates(inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa159f25",
   "metadata": {},
   "source": [
    "Now let's remove outliers with the Interquartile Range method. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9aabba55",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to calculate IQR and remove outliers\n",
    "def remove_outliers(collision_data,column):\n",
    "    Q1 = collision_data[column].quantile(0.25)  # 25th percentile\n",
    "    Q3 = collision_data[column].quantile(0.75)  # 75th percentile\n",
    "    IQR = Q3 - Q1                   # Inter-Quartile Range\n",
    "    \n",
    "    # Define lower and upper bounds\n",
    "    lower_bound = Q1 - 1.5 * IQR\n",
    "    upper_bound = Q3 + 1.5 * IQR\n",
    "    \n",
    "    # Filter out rows with outliers\n",
    "    return collision_data[(collision_data[column] >= lower_bound) & (collision_data[column] <= upper_bound)]\n",
    "\n",
    "# List of numeric columns\n",
    "numeric_columns = collision_data.select_dtypes(include=['float64', 'int64']).columns\n",
    "\n",
    "# Remove outliers using the IQR method\n",
    "for column in numeric_columns:\n",
    "    collision_data = remove_outliers(collision_data, column)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91f3c091",
   "metadata": {},
   "source": [
    "That is all for data cleaning, let's see the new size of the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85006fab",
   "metadata": {},
   "outputs": [],
   "source": [
    "collision_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf0db5f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# View DataFrame\n",
    "collision_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5199150d",
   "metadata": {},
   "source": [
    "## Exploratory Data Analysis of Collisions Data\n",
    "\n",
    "The exploratory data analysis (EDA) for the collisions dataset aims to uncover patterns, trends, and insights into the distribution and characteristics of collisions across Toronto. By examining factors such as temporal trends, spatial distributions, road conditions, light conditions, and driver behavior, we can better understand the key contributors to traffic incidents. This analysis provides a foundational understanding of the dataset, enabling us to identify high-risk areas, seasonal patterns, and other influential factors that impact collision frequency. Through this EDA, we aim to generate actionable insights that can inform road safety initiatives, policy decisions, and future predictive modeling efforts."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57a7fa97",
   "metadata": {},
   "source": [
    "### Extracted Time Features from Collision Data\n",
    "\n",
    "After converting the `DATE` column to a proper datetime format, we extracted several time-based attributes to analyze collision patterns more effectively. The following features were derived:\n",
    "\n",
    "1. **Year**: The year in which the collision occurred.\n",
    "2. **Month**: The month of the collision (1 = January, 12 = December).\n",
    "3. **Day of the Week**: The name of the day (e.g., Monday, Tuesday).\n",
    "4. **Hour**: The hour of the day when the collision occurred (0-23).\n",
    "\n",
    "These features allow for temporal analysis of the data, such as identifying trends over years, seasonal patterns, or the most common days and hours for collisions. For example, examining collisions by the day of the week can help identify whether weekdays or weekends experience more accidents, while the hour attribute can reveal peak traffic or high-risk hours.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d9fa12f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure the DATE column is in datetime format\n",
    "collision_data['DATE'] = pd.to_datetime(collision_data['DATE'], errors='coerce')\n",
    "\n",
    "# Extract year, month, day of the week, and hour\n",
    "collision_data['Year'] = collision_data['DATE'].dt.year\n",
    "collision_data['Month'] = collision_data['DATE'].dt.month\n",
    "collision_data['DayOfWeek'] = collision_data['DATE'].dt.day_name()\n",
    "collision_data['Hour'] = collision_data['DATE'].dt.hour\n",
    "\n",
    "# Display the updated DataFrame\n",
    "print(collision_data.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61ef6fc0",
   "metadata": {},
   "source": [
    "\n",
    "These extracted features will support predictive modeling, such as forecasting collisions by time or understanding time-based risk factors within each ward."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73a141de",
   "metadata": {},
   "source": [
    "### Yearly Collision Trends Analysis\n",
    "\n",
    "Understanding the yearly trends in collision data is essential for identifying patterns over time, such as increases or decreases in the number of collisions. This analysis can highlight the effectiveness of safety interventions or reveal emerging risks that require attention. By plotting the number of collisions for each year, we can assess long-term trends, which can inform city planning, policy-making, and resource allocation for traffic safety improvements. The following code calculates and visualizes the yearly collision trends from the dataset to provide these insights.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5138dec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot collisions per year\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.countplot(x='Year', data=collision_data)\n",
    "plt.title('Yearly Collision Counts')\n",
    "plt.xlabel('Year')\n",
    "plt.ylabel('Number of Collisions')\n",
    "plt.xticks(rotation=45)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "371d4749",
   "metadata": {},
   "source": [
    "\n",
    "### Monthly Collision Patterns\n",
    "\n",
    "Understanding the distribution of collisions across months provides valuable insights into seasonal trends and potential environmental or behavioral factors influencing road safety. Analyzing monthly collision patterns can help identify peak months for collisions, enabling policymakers and urban planners to implement targeted safety measures during high-risk periods. Below, we will explore the monthly collision trends using the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "734bf422",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot collisions per month\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.countplot(x='Month', data=collision_data)\n",
    "plt.title('Monthly Collision Counts')\n",
    "plt.xlabel('Month')\n",
    "plt.ylabel('Number of Collisions')\n",
    "plt.xticks(range(0, 12), ['Jan', 'Feb', 'Mar', 'Apr', 'May', 'Jun',\n",
    "                          'Jul', 'Aug', 'Sep', 'Oct', 'Nov', 'Dec'], rotation=45)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2210242c",
   "metadata": {},
   "source": [
    "#### Relationship between Month and Year\n",
    "To explore the monthly collision trends across individual years from 2020 to 2024, we will use a series of bar plots, one for each year. This approach allows us to focus on the collision counts for each month within a given year, enabling a more granular examination of seasonal patterns and year-specific variations. By visualizing how collisions fluctuate month-by-month for each year, we can identify recurring trends, seasonal peaks, or anomalies specific to certain years. This targeted analysis is particularly useful for tailoring safety interventions and resource allocation to address year-specific collision patterns effectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51fc7c70",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure the data has necessary columns: 'Year', 'Month' and collision counts\n",
    "collision_data['Month'] = collision_data['DATE'].dt.month\n",
    "collision_data['Year'] = collision_data['DATE'].dt.year\n",
    "\n",
    "# Filter data to include only years from 2021 to 2024\n",
    "collision_filtered = collision_data[(collision_data['Year'] >= 2020) & (collision_data['Year'] <= 2023)]\n",
    "\n",
    "# Group the data by 'Year' and 'Month' to count the number of collisions per month per year\n",
    "monthly_trends = collision_filtered.groupby(['Year', 'Month']).size().reset_index(name='Collision_Count')\n",
    "\n",
    "# Create separate plots for each year showing monthly collision counts\n",
    "g = sns.FacetGrid(monthly_trends, col=\"Year\", col_wrap=2, height=4, sharey=True)\n",
    "g.map(sns.barplot, \"Month\", \"Collision_Count\", order=range(1, 13))\n",
    "\n",
    "# Set titles and labels\n",
    "g.set_axis_labels(\"Month\", \"Collision Count\")\n",
    "g.set_titles(\"{col_name}\")\n",
    "g.fig.suptitle(\"Monthly Collision Trends by Year (2020-2023)\", y=1.02)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2170a3e4",
   "metadata": {},
   "source": [
    "### Day of the Week Collision Pattern\n",
    "\n",
    "To further analyze collision patterns, we will examine how collisions are distributed across different days of the week. This analysis can help uncover trends related to weekly activities, such as increased traffic on weekdays due to work commutes or higher collision counts on weekends due to leisure travel. By visualizing these patterns, we can better understand how daily behaviors influence road safety and identify high-risk days for implementing targeted interventions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "782cb9cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot collisions by day of the week\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.countplot(x='DayOfWeek', data=collision_data, order=['Monday', 'Tuesday', 'Wednesday', \n",
    "                                                    'Thursday', 'Friday', 'Saturday', 'Sunday'])\n",
    "plt.title('Collisions by Day of the Week')\n",
    "plt.xlabel('Day of the Week')\n",
    "plt.ylabel('Number of Collisions')\n",
    "plt.xticks(rotation=45)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64437286",
   "metadata": {},
   "source": [
    "###  Hour Collision Patterns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d086d75f",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ac29909",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "52554b08",
   "metadata": {},
   "source": [
    "## Import Ward Data\n",
    "\n",
    "To predict the number of collisions in Toronto, the collision data must be spatially joined with the ward boundaries to determine which ward each collision occurred in. This process involves using the geographic coordinates (latitude and longitude) from the collision data and mapping them to the corresponding ward polygons in the ward dataset. After assigning collisions to their respective wards, we can aggregate the number of collisions per ward to identify trends and high-risk areas. This data can then be enriched with additional ward-specific features, such as population, road density, and traffic volume, to build a predictive model. By training a machine learning model with these features, we can forecast collision counts and provide actionable insights for city planning and traffic safety improvements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "425f78d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the shapefile\n",
    "ward_shapefile_path = \"WARD_WGS84.shx\" \n",
    "ward_data = gpd.read_file(ward_shapefile_path.replace(\".shx\", \".shp\"))  # Replace with .shp extension\n",
    "\n",
    "# Create a base map\n",
    "map_wards = folium.Map(location=[43.7, -79.4], zoom_start=11)  # Adjust to center on Toronto\n",
    "\n",
    "# Add the ward shapefile to the map\n",
    "ward_data_json = ward_data.to_json()  # Convert GeoDataFrame to GeoJSON\n",
    "folium.GeoJson(ward_data_json, name=\"Wards\").add_to(map_wards)\n",
    "\n",
    "# Display the map\n",
    "map_wards"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe70380b",
   "metadata": {},
   "source": [
    "## Ward Data Analysis "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4084a487",
   "metadata": {},
   "outputs": [],
   "source": [
    "# View geoDataFrame\n",
    "ward_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c3933fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the number of columns and rows\n",
    "ward_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bded8ba0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the columns in DataFrame\n",
    "ward_data.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d199458c",
   "metadata": {},
   "source": [
    "The columns describe the following:\n",
    "- 'AREA_ID': A unique identifier for each ward in the dataset.\n",
    "- 'AREA_TYPE': Indicates the type of area (all is WD18 for wards in the 2018 model of Toronto).\n",
    "- 'AREA_S_CD': A short code representing the ward, likely numeric.\n",
    "- 'AREA_L_CD': A longer code that may provide additional context or differentiation for the ward.\n",
    "- 'AREA_NAME': The official name of the ward.\n",
    "- X,y : coordiantes in a projected coordinate system.\n",
    "- Longitude, Latitude: Geographic lat and long of the ward's approximate center.\n",
    "  \n",
    "Let's explore the differences between 'AREA_S_CD' and 'AREA_L_CD'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2374d32",
   "metadata": {},
   "outputs": [],
   "source": [
    "ward_data['AREA_S_CD'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe2627c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "ward_data['AREA_L_CD'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e879c010",
   "metadata": {},
   "source": [
    "There are no differences between the two, therefore drop 'AREA_L_CD'. \n",
    "\n",
    "These other features will also be dropped:\n",
    "- Drop 'AREA_ID' and let 'AREA_S_CD' act as the only identifier of the wards.\n",
    "- Drop 'AREA_TYPE' as all wards are of type WD18, therefore, no new information or classification is provided. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45523812",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop unnecessary columns \n",
    "ward_data = ward_data.drop(columns = ['AREA_ID', 'AREA_S_CD', 'AREA_TYPE', 'X', 'Y', 'LONGITUDE', 'LATITUDE'], errors='ignore')\n",
    "\n",
    "# Check the columnsare dropped in geoDataFrame\n",
    "ward_data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40e65ff2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check data types per column\n",
    "print(ward_data.dtypes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "545d439d",
   "metadata": {},
   "source": [
    "## Ward Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76eb91e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for missing values\n",
    "print(ward_data.isnull().sum())\n",
    "# Check for duplicates\n",
    "ward_data.duplicated().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b6f5a9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# View geoDataFrame\n",
    "ward_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20e6c9de",
   "metadata": {},
   "source": [
    "The results indicate that there are no missing values or duplicate rows across key attributes like AREA_L_CD, AREA_NAME, X, Y, LONGITUDE, LATITUDE, and geometry. This suggests that the dataset is clean and ready for subsequent analysis, ensuring the integrity of the data for spatial operations and further processing."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa0d799e",
   "metadata": {},
   "source": [
    "## Overlay Collision Data onto Ward Data \n",
    "\n",
    "\n",
    "Overlaying the collision data onto the ward data is essential for spatial analysis and understanding where collisions are occurring within the city. By mapping each collision to a specific ward, we can identify patterns and trends in collision occurrences relative to geographic boundaries. This allows for aggregating the number of collisions per ward, which is critical for targeted analysis, policy-making, and resource allocation. For example, high-collision wards can be prioritized for road safety improvements or public awareness campaigns. Additionally, integrating collision data with ward-specific attributes such as population, traffic volume, or road density enables more accurate predictive modeling and helps address traffic safety issues more effectively."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8dcf0ae",
   "metadata": {},
   "source": [
    "\n",
    "Converting collision data into a GeoDataFrame is essential for spatial analysis, as it allows for operations like spatial joins, overlays, and mapping. By creating a geometry column from LATITUDE and LONGITUDE, each collision is represented as a precise point in space. Assigning a Coordinate Reference System (CRS), such as EPSG:4326 (WGS84), ensures the data aligns accurately with other spatial datasets, like ward boundaries. This conversion enables mapping collisions to specific wards, visualizing spatial patterns, and ensuring compatibility with geospatial tools, making it a critical step for reliable and accurate analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af6f2507",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert collision data to GeoDataFrame\n",
    "collision_data[\"geometry\"] = collision_data.apply(lambda row: Point(row[\"LONGITUDE\"], row[\"LATITUDE\"]), axis=1)\n",
    "collision_data = gpd.GeoDataFrame(collision_data, geometry=\"geometry\", crs=\"EPSG:4326\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "634a9434",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure CRS matches\n",
    "if collision_data.crs != ward_data.crs:\n",
    "    collisio_data = collision_data.to_crs(ward_data.crs)\n",
    "\n",
    "# Spatial join to find which ward each collision occurred in\n",
    "collision_with_wards = gpd.sjoin(collision_data, ward_data, how=\"left\", op=\"within\")\n",
    "\n",
    "# Display the results\n",
    "print(collision_with_wards.head())\n",
    "\n",
    "# Visualize on a map\n",
    "map_wards = folium.Map(location=[43.7, -79.4], zoom_start=11)\n",
    "ward_data_json = ward_data.to_json()\n",
    "folium.GeoJson(ward_data_json, name=\"Wards\").add_to(map_wards)\n",
    "\n",
    "# Add collision points\n",
    "for _, row in collision_data.iterrows():\n",
    "    folium.CircleMarker(\n",
    "        location=[row.geometry.y, row.geometry.x],\n",
    "        radius=3,\n",
    "        color=\"red\",\n",
    "        fill=True,\n",
    "        fill_color=\"red\",\n",
    "        fill_opacity=0.6\n",
    "    ).add_to(map_wards)\n",
    "\n",
    "map_wards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbfdf428",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform the spatial join to find which ward each collision occurred in\n",
    "collision_with_wards = gpd.sjoin(collision_data, ward_data, how=\"left\", op=\"within\")\n",
    "\n",
    "# Drop unnecessary geometry columns for tabular display\n",
    "collision_table = collision_with_wards.drop(columns=[\"geometry\", \"index_right\"])\n",
    "\n",
    "# Display the table in the notebook\n",
    "collision_table.tail()  # Display first few rows of the table"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "444fe7b5",
   "metadata": {},
   "source": [
    "## Exploratory Data Analysis with Collision and Ward Data (2021 to 2024)\n",
    "\n",
    "The exploratory data analysis (EDA) linking collision data to ward data focuses on understanding how traffic incidents are distributed across different geographic regions. By spatially associating each collision with its corresponding ward, we aim to identify high-risk areas, observe regional patterns, and analyze variations in collision frequency. This analysis allows us to explore how factors such as road conditions, traffic density, and infrastructure might vary across wards, influencing collision rates. By integrating collision and ward data, we can generate valuable insights to prioritize safety improvements, allocate resources effectively, and support data-driven decision-making at a regional level."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d697bab",
   "metadata": {},
   "source": [
    "### Collisions Across Wards from 2021 to 2024\n",
    "\n",
    "When analyzing the number of collisions within each ward, patterns may emerge regarding high-collision areas versus wards with lower collision counts. This type of analysis is critical for understanding geographic trends in collision frequency. High-collision wards might correspond to regions with greater population density, higher traffic volumes, or complex road infrastructure, while low-collision wards might indicate less urbanized or lower-traffic areas."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "536f34f9",
   "metadata": {},
   "source": [
    "Before diving into this section, we aim to refine our collision dataset by filtering for data between the years 2021 and 2024, providing a recent and focused analysis. This filtered dataset will then be spatially joined with ward boundaries to associate each collision with its respective ward. By organizing the results chronologically by ward number and calculating the percentage distribution of collisions across wards, we gain a clear understanding of collision patterns in different regions. This step is essential for identifying high-collision areas and drawing meaningful insights to inform regional safety strategies and resource allocation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a54efcd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the 'DATE' column to datetime for filtering\n",
    "collision_data[\"DATE\"] = pd.to_datetime(collision_data[\"DATE\"])\n",
    "\n",
    "# Filter collisions from 2021 to 2024\n",
    "collision_2021_2024 = collision_data[(collision_data[\"DATE\"].dt.year >= 2021) & (collision_data[\"DATE\"].dt.year <= 2024)]\n",
    "\n",
    "# Perform the spatial join to associate collisions with wards\n",
    "collision_with_wards = gpd.sjoin(collision_2021_2024, ward_data, how=\"left\", predicate=\"within\")\n",
    "\n",
    "# Clean up the resulting DataFrame by dropping unnecessary geometry and index columns\n",
    "collision_table = collision_with_wards.drop(columns=[\"geometry\", \"index_right\"])\n",
    "\n",
    "# Group collisions by ward number and name, maintaining chronological order\n",
    "ward_collision_counts = collision_table.groupby([\"AREA_L_CD\", \"AREA_NAME\"]).size().reset_index(name=\"Number of Collisions\")\n",
    "\n",
    "# Sort wards in ascending order by ward number\n",
    "ward_collision_counts = ward_collision_counts.sort_values(by=\"AREA_L_CD\")\n",
    "\n",
    "# Calculate the percentage of collisions for each ward\n",
    "total_collisions = ward_collision_counts[\"Number of Collisions\"].sum()\n",
    "ward_collision_counts[\"Percentage of Collisions\"] = (ward_collision_counts[\"Number of Collisions\"] / total_collisions) * 100\n",
    "\n",
    "# Display the summary DataFrame\n",
    "ward_collision_counts.head(25)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "291efd77",
   "metadata": {},
   "source": [
    "### Collisions across all Wards every month from 2021 to 2024"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "160c2f1b",
   "metadata": {},
   "source": [
    "To gain a deeper understanding of collision patterns across different wards over time, we will generate both a tabular and visual representation of the number of collisions occurring in each ward per month from 2021 to 2023. The table provides a clear breakdown of collision counts by ward, organized by year and month, offering a detailed view of temporal trends. Following this, a bar graph visually highlights these trends, enabling easy comparison of collision counts across wards and months. These insights are essential for identifying high-risk periods and areas, aiding in targeted road safety interventions and policy-making."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f66e37b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the 'DATE' column to datetime for filtering\n",
    "collision_data[\"DATE\"] = pd.to_datetime(collision_data[\"DATE\"])\n",
    "\n",
    "# Filter collisions from 2021 to 2023\n",
    "collision_filtered = collision_data[\n",
    "    (collision_data[\"DATE\"].dt.year >= 2021) & (collision_data[\"DATE\"].dt.year <= 2023)\n",
    "]\n",
    "\n",
    "# Extract year and month from the 'DATE' column\n",
    "collision_filtered[\"Year\"] = collision_filtered[\"DATE\"].dt.year\n",
    "collision_filtered[\"Month\"] = collision_filtered[\"DATE\"].dt.month\n",
    "\n",
    "# Perform the spatial join to associate collisions with wards\n",
    "collision_with_wards = gpd.sjoin(collision_filtered, ward_data, how=\"left\", predicate=\"within\")\n",
    "\n",
    "# Group collisions by ward number, ward name, and month\n",
    "ward_monthly_collisions = (\n",
    "    collision_with_wards.groupby([\"AREA_L_CD\", \"AREA_NAME\", \"Year\", \"Month\"])\n",
    "    .size()\n",
    "    .reset_index(name=\"Number of Collisions\")\n",
    ")\n",
    "\n",
    "# Pivot the table to display months as columns\n",
    "ward_monthly_pivot = ward_monthly_collisions.pivot_table(\n",
    "    index=[\"AREA_L_CD\", \"AREA_NAME\"],\n",
    "    columns=[\"Year\", \"Month\"],\n",
    "    values=\"Number of Collisions\",\n",
    "    fill_value=0\n",
    ")\n",
    "\n",
    "# Flatten multi-level columns for readability\n",
    "ward_monthly_pivot.columns = [f\"{year}-{month:02}\" for year, month in ward_monthly_pivot.columns]\n",
    "\n",
    "# Reset the index to make it a clean DataFrame\n",
    "ward_monthly_pivot.reset_index(inplace=True)\n",
    "\n",
    "# Display the first few rows of the resulting DataFrame\n",
    "ward_monthly_pivot.head(25)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
